import json
import os

import torch
from datasets import load_dataset
from modelscope import snapshot_download
from peft import LoraConfig, get_peft_model
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    TrainerCallback,
    TrainerState,
    TrainerControl,
    EarlyStoppingCallback,
    IntervalStrategy
)
from transformers.trainer_utils import SaveStrategy

# ==================== 1ã€å®šä¹‰å˜é‡ ====================
current_dir = os.path.dirname(os.path.abspath(__file__))
data_path = os.path.join(current_dir, "../../datas/code_alpaca.json")
output_dir = os.path.abspath(os.path.join(current_dir, "./output/qwen2.5-7b-qlora-2"))
os.makedirs(output_dir, exist_ok=True)

assert os.path.exists(data_path), f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{data_path}"

model_name = snapshot_download("Qwen/Qwen2.5-Coder-7B-Instruct")

# ==================== 2ã€é‡åŒ–åŠ è½½æ¨¡å‹ï¼ˆQLoRAï¼‰ ====================
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# ==================== 3ã€LoRA é…ç½® ====================
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "up_proj", "down_proj", "gate_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()


# ==================== 4ã€æ˜¾å­˜ç›‘æ§ ====================
def print_gpu_memory(prefix=""):
    if torch.cuda.is_available():
        print(f"\n{prefix} GPU æ˜¾å­˜å ç”¨ï¼ˆGBï¼‰ï¼š")
        print(f"å·²åˆ†é…: {torch.cuda.memory_allocated() / 1024 ** 3:.2f}")
        print(f"æ€»é¢„ç•™: {torch.cuda.memory_reserved() / 1024 ** 3:.2f}")
    else:
        print("\nå½“å‰æ²¡æœ‰æ£€æµ‹åˆ° GPUï¼")


print_gpu_memory("è®­ç»ƒå‰")

# ==================== 5ã€åŠ è½½ä¸æ ¼å¼åŒ–æ•°æ® ====================
dataset = load_dataset("json", data_files=data_path)
full_dataset = dataset["train"]

# è®­ç»ƒé›†å’ŒéªŒè¯é›†åˆ’åˆ†
split_dataset = full_dataset.train_test_split(test_size=0.05, seed=42)
train_data = split_dataset["train"]
eval_data = split_dataset["test"]

print(f"\næ•°æ®é›†åˆ’åˆ†ï¼š")
print(f"è®­ç»ƒé›†æ ·æœ¬æ•°ï¼š{len(train_data)}")
print(f"éªŒè¯é›†æ ·æœ¬æ•°ï¼š{len(eval_data)}")


# æ”¯æŒæ‰¹å¤„ç†ã€è¿‡æ»¤ç©ºæ ·æœ¬ï¼Œé˜²æ­¢ padding_mask æŠ¥é”™
def format_batch(batch):
    prompts = []
    for inst, inp, outp in zip(batch["instruction"], batch["input"], batch["output"]):
        if not inst or not outp:  # è·³è¿‡ç©ºæ ·æœ¬
            continue

        if inp:
            text = (
                f"### æŒ‡ä»¤:\n{inst.strip()}\n\n"
                f"### è¾“å…¥:\n{inp.strip()}\n\n"
                f"### å›å¤:\n{outp.strip()}"
            )
        else:
            text = (
                f"### æŒ‡ä»¤:\n{inst.strip()}\n\n"
                f"### å›å¤:\n{outp.strip()}"
            )

        prompts.append(text)

    tokenized = tokenizer(
        prompts,
        truncation=True,
        max_length=1024,
        padding="max_length",
        return_tensors=None,
    )
    return tokenized


tokenized_train = train_data.map(format_batch, batched=True, remove_columns=train_data.column_names)
tokenized_eval = eval_data.map(format_batch, batched=True, remove_columns=eval_data.column_names)

# ğŸ”§ æ£€æŸ¥ç©ºæ ·æœ¬
print(f"Tokenized Train: {len(tokenized_train)}, Eval: {len(tokenized_eval)}")

# ==================== 6ã€è®­ç»ƒé…ç½® ====================
training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=8,
    warmup_steps=50,
    num_train_epochs=3,
    learning_rate=2e-5,
    weight_decay=0.01,
    fp16=True,
    eval_strategy=IntervalStrategy.STEPS,
    eval_steps=80,
    save_strategy=SaveStrategy.STEPS,
    save_steps=80,
    save_total_limit=3,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,

    logging_steps=50,
    report_to="none",
    prediction_loss_only=True,
)

data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)


# ==================== 7ã€è‡ªå®šä¹‰å›è°ƒ ====================
class MemoryMonitorCallback(TrainerCallback):
    def on_log(self, args, state: TrainerState, control: TrainerControl, **kwargs):
        step = state.global_step
        if step % 10 == 0 or step == 1:
            print(f"\n========== Step {step} ==========")
            if torch.cuda.is_available():
                print(f"å·²åˆ†é…æ˜¾å­˜ = {torch.cuda.memory_allocated() / 1024 ** 3:.2f} GB")
                print(f"æ€»é¢„ç•™æ˜¾å­˜ = {torch.cuda.memory_reserved() / 1024 ** 3:.2f} GB")

    def on_evaluate(self, args, state: TrainerState, control: TrainerControl, metrics=None, **kwargs):
        if metrics:
            eval_loss = metrics.get("eval_loss", 0)
            if len(state.log_history) > 1:
                train_loss = state.log_history[-2].get("loss", 0)
                print(f"\nã€è¿‡æ‹Ÿåˆæ£€æµ‹ã€‘")
                print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}")
                print(f"  éªŒè¯æŸå¤±: {eval_loss:.4f}")
                if eval_loss - train_loss > 0.5:
                    print(f"è­¦å‘Šï¼šå¯èƒ½å‡ºç°è¿‡æ‹Ÿåˆï¼")


# ==================== 8ã€å¯åŠ¨è®­ç»ƒ ====================
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval,
    data_collator=data_collator,
    callbacks=[
        # ç›‘æ§è®­ç»ƒè¿‡ç¨‹ä¸­çš„å†…å­˜ï¼ˆæ˜¾å­˜ï¼‰ä½¿ç”¨æƒ…å†µï¼Œä¾¿äºæ’æŸ¥ OOMï¼ˆå†…å­˜æº¢å‡ºï¼‰é—®é¢˜
        MemoryMonitorCallback(),
        # æ—©åœç­–ç•¥
        EarlyStoppingCallback(
            early_stopping_patience=3,
            early_stopping_threshold=0.001
        )
    ],
)

print("\nå¼€å§‹è®­ç»ƒ...")

trainer.train()

# ==================== 9ã€ä¿å­˜ LoRA æƒé‡ ====================
print("\nä¿å­˜ LoRA æƒé‡...")
trainer.model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)
print(f"âœ“ LoRA æƒé‡å·²ä¿å­˜åˆ°ï¼š{output_dir}")

history_file = os.path.join(output_dir, "training_history.json")
with open(history_file, 'w', encoding='utf-8') as f:
    json.dump(trainer.state.log_history, f, indent=2, ensure_ascii=False)
print(f"âœ“ è®­ç»ƒå†å²å·²ä¿å­˜åˆ°ï¼š{history_file}")

# ==================== 10ã€è®­ç»ƒç»“æŸ ====================
print_gpu_memory("è®­ç»ƒç»“æŸ")
print("\n" + "=" * 50)
print("è®­ç»ƒå®Œæˆï¼")
print(f"LoRA adapter ä¿å­˜åœ¨: {output_dir}")
print("=" * 50)
